var smart =	require('sk/core/smart').smart,
SmartCacher        = smart.Cacher,
getImageSize = require('./getImageSize'),
findLocation = require('psk/node/io/net').findLocation,
getFrequency = require('sk/core/math').getFrequency,
Structr = require('structr'),
logger = require('winston').loggers.get('feeds.stream.core'),
sprintf = require('sprintf').sprintf,
outcome = require('outcome'),
dumpOld = require('../utils/dumpOld');


var FeedDelegate = function(plugin, dbManager, SpiceAppFeedInfoModel, c_feedData)
{

	this.plugin = plugin;
	
	var MAX_TTL       = 1000 * ( plugin.appParams.maxFeedCacheTime || (60 * 60 * 12)),//12 hours TTL max
	expandStreamUrls  = plugin.appParams.expandStreamUrls,
	MIN_TTL           = 10000, //10 seconds minimum             
	DEFAULT_TTL       = Math.min(60 * 60 * 1000, MAX_TTL),
	INCREASE_PERC_TTL = 1.25,
	MAX_FEED_AGE	  = 14,
	MAX_FEED_RECORDS  = plugin.appParams.mxaStreamFeedRecords || 30,//max of N items                             
	self = this;
	
	logger.info(sprintf('starting feed delegate with a max TTL of %s MS, max feed records of %s', MAX_TTL, MAX_FEED_RECORDS));
	

	                                     
	function startSlave()
	{

		logger.verbose('starting feed.stream.core slave')

		plugin.router.on({

			/**                                                                                                                 
			 */


			'push -pull -hook thyme/ready': function()
			{                      
				this.from.push('thyme/worker', { queue: process.env.SLAVE_APP_NAME, path: 'stream/next/feed', max: 30, timeout: 20000, lockTTL: 20000, tries: -1 });
			},

			/**
			 */

			'pull -hook stream/next/feed': function(req, res)
			{                    
				var feed = this.flattenData();                                    
				
				console.log('stream next feed: %s', feed.feedId);                 
				
				plugin.router.request('feed/identify').query({ feed: feed.feedId }).success(function(response)
				{
					                                                     
					plugin.loader.load('stream', { skipFetch: true, feeds: this.identifiedFeeds, wait: true, ignoreWatch: true }, function()
					{                                                       
						console.log('Loaded %s', feed.feedId);

						getFeedInfo(feed.feedId, false, function(err, info)
						{
							if(!info)
							{
								res.end();

								return console.error('feed %s does not exist', feed.feedId);
							}

							function onReady(info)
							{	                                
								res.end(info.watchUntil.getTime() > new Date().getTime() ? { sendAt: info.cacheUntil.getTime() } : null);
							}

							//okay, something went wrong. increase TTL
							if(info.cacheUntil.getTime() < new Date().getTime())
							{                                    
								updateFeedStats(info._id, [], undefined, onReady);
							}
							else
							{
								onReady(info);
							}
						});      
					});
				
				}).pull();
			}
		});
	}	



	
	plugin.router.on({

		/**
		 */

		'push -pull init/slave': startSlave,

		/**
		 */


		'push watch/feed': function(data)
		{
			console.log('watch feed %s', data.feedId);

			//watch for duration of time, then stop watching
			self.watchFeed(data.feedId, data.ttl, function(){});
		},


		/**
		 */

		'push unwatch/feed': function(data)
		{
			console.log('unwatch feed %s', data.feedId);

			//set ttl to 1 ms, unwatching the feed essentially
			self.watchFeed(data.feedId, 1, function(){});
		}
	});
	
	function clearCache(feedId)
	{
		// dbManager.promotionsDelegate.clearCache(feedId);
	}

	//adds the cached feed data based on the ID given, but it 
	//also returns back the time to live. Maybe, if the number is
	//low enough, we load data automatically (like google reader).
	function addFeedData(feedId, useable, callback)
	{   		 
		//regardless of the size of content, we still need to calculate the TTL so that we can max-out the TTL over night.
		//The TTL however is also determined by the the amount of data at a given time (-12 hours?). 
		updateFeedStats(feedId, useable, undefined, function (feedStats)
		{                        
			 //if(!useable.isUseable) throw new Error('the data must be checked for duplicates before it can be added.');
			if(useable.length == 0) return callback([]);  


			//only permit a certain amount of feeds into the database. This may happen
			//when items are loaded in for the first time from services such as twitter. We allow a large
			//amount of feeds to be loaded for promoting content.
			if(useable.length > MAX_FEED_RECORDS)
			{
				useable.splice(0,useable.length - MAX_FEED_RECORDS)
			}                    



			var oldest = useable[0];
			
			
			function update()
			{
				//we CACHE data. we don't store it. Remove any feeds that are older than the current feeds, but ALWAYS keep N amount cached
				//in memory so we have the same page length.
				c_feedData.remove({ _id: feedId, createdAt: { $lt: oldest.createdAt } }, { limit: MAX_FEED_RECORDS - (MAX_FEED_RECORDS - useable.length) }, function ()
				{      
					//add the new data 
					c_feedData.insert(useable, { emit: true }, function ()
					{
						callback(useable, feedStats);


						//clear the cache now.
						clearCache(feedId);
					}); 
				})
			}
			
			//taxing depending on the service TODO: this shouldn't be here....
			if(expandStreamUrls)
			{
				logger.verbose('expanding stream urls');
				var i = useable.length;
				
				function checkDone()
				{
					if(!(--i)) update();
				}
				
				useable.forEach(function(streamItem)
				{
					if(!streamItem.link || !streamItem.media) return checkDone();
					
					findLocation(streamItem.link, function(link)
					{
						if(!link) link = streamItem.link;
						
						for(var i = streamItem.media.length; i--;)
						{
							var md = streamItem.media[i];
							
							// if(md.link == streamItem.link)
							{
								plugin.request('nectarize').query(link).success(function(media)
								{
									if(!media || !media.length) return checkDone();
									
									var newMd = media[0];
									
									newMd.match = md.link;
									// streamItem.media.splice(i,1);
									streamItem.media.splice(i,1,newMd);


									if(newMd.type == 'image' && (!newMd.regular.width || !newMd.regular.height))
									{
										getImageSize(newMd.regular.link, function(result)
										{
											if(!result || result.error)
											{
												streamItem.media.slice(streamItem.media.indexOf(newMd), 1);
											}
											else
											{
												newMd.regular.width = result.width;
												newMd.regular.height = result.height;
											}
											checkDone();
										});

									}
									else
									{
										checkDone();
									}
								}).pull();
								
								break;
							}
						}
					})
					
				});
				
			}
			else
			{
				update();
			}
			
		});
	}
	
	var processingFeeds = {};
	
	//walks the loader through the stream data. 
	this.walkThroughStream = function(feedId, callbacks)
	{ 

		//it's locked, because we're still workn' it. 
		if(processingFeeds[feedId])
		{
			logger.info(sprintf('%s is probably still loading', feedId));
			return callbacks.fail();
		}
		
		processingFeeds[feedId] = true;
		
		function finish(fail, useable)
		{
			//if the feed isn't processing, it's most likely already returned data. THIS chunk will 
			//get hit when a feed is loaded multiple times over a single request.
			if(!processingFeeds[feedId]) return;
			
			delete processingFeeds[feedId];
			if(fail) callbacks.fail();
			if(useable) callbacks.finish(useable);
		}


		
		self.cachedDataIsOld(feedId, function(isOld, isNew)
		{

			if(!isOld)
			{
				logger.info(sprintf('%s is still cached', feedId));
				return finish(true);
			}
			
			
			//load the content since the data is old
			callbacks.load(isNew, function (newFeedItems)
			{
				if(!newFeedItems)
				{
					//this is verbose, because it can be quite common, especially with services that have a cap on the number of requests / hour          
					logger.info(sprintf('unable to load data for %s setting to default ttl', feedId));
					
					//this could be problematic... we WANT to cache the data regardless of what we get back. It could be
					//that the item parsed incorrectly, or there isn't any data. In which case, we still need to set the MAX_TTL
					setMaxTTL(feedId);
					return finish(true);
				}
				
				//make sure there's no overlap
				getUseableData(feedId, newFeedItems, function (err, useable)
				{
					if(err || !useable)
					{
						logger.warn(sprintf('unexpected error when trying to get usable data from %s', feedId));
						return finish(true);
					}
					
					//process the useable data even further (the heavier shit)
					callbacks.processUseableFeedItems(useable);
					
					
					//finally, we need to add the feeds in the database
					addFeedData(feedId, useable, function ()
					{
						finish(false, useable);
					});
					
				})
			});
		})
	}
	
	this.countFeedData = function(search, callback)
	{
		SpiceAppFeedInfoModel.count(search, callback);
	}

	//returns data stored in the databased based on the feed ID's given
	this.getFeedData = function(search, params, callback)
	{
		if(!callback)
		{
			callback = params;
			params   = {};
		}

		var page  = params.page || 0;
		var count = Math.min(params.count || MAX_FEED_RECORDS, 100);

		//to do later: {skip:params.count*params.page,limit:params.count,sort:[['createdAt',-1]]}
		c_feedData.find(search, { skip: page * count, limit: count, sort: [['createdAt',-1]] }, function (err, cursor)
		{
			cursor.toArray(function(err, result)
			{
				callback(err, result);
			});
		})
	}
	
	
	//for debugging
	this.getAllFeedData = function(callback)
	{
		c_feedData.find(function(err, cursor)
		{
			callback(err, cursor);
		})
	}
                                                                  
	//N Users * N Avg feeds * N new posts / feed with links 
	//1000000 entries ~ 15 MB ram 
	var FeedTTLCacher = new SmartCacher(1000, 'FeedTTLCacher', true);
	
	
	//returns true if TTL for the feed ID exceeds the cache time
	this.cachedDataIsOld = function (feedId, callback)
	{                                                          
		//take it from memory vs hitting the database
		//NOTE: there's a method called getTimeLeft, but we don't want that. sending back
		//"time left" will allow clients to synchronize the next time content is refresh, and holy shit
		//that's probably a really, REALLY bad idea.
		var ttl = Number(FeedTTLCacher.get(feedId));
		
		
		logger.verbose(sprintf('checking if %s is old', feedId));
		/*if(ttl)g
		{                
			return callback(!ttl, ttl); //NO. it's NOT old.   
		}*/
		
		                                                                             
		//get the feed stats IF they exist. OTHERWISE we want to go ahead send back true                 
		getFeedInfo(feedId, function (err, feed)
		{
			callback(feed ? feed.isOld : true, !feed || feed.lockCount <= 1);
		})
	}
	
	
	//watches a feed for a duration of time before stopping 
	this.watchFeed = function (feedId, watchTTL, callback)
	{
		if(feedId instanceof Array)
		{
			return feedId.forEach(function (feed)
			{
				self.watchFeed(feed, watchTTL, callback);
			})
		}
		
		
		//need to have a "validateFeedId" here.
		if(!feedId || feedId.split(':').length < 3)
		{
			return logger.warn(sprintf('cannot watch feed %s because it\'s in the wrong format.', feedId));
		}

		
		updateFeedStats(feedId, undefined, watchTTL || DEFAULT_TTL, function(feedData)
		{	
			logger.verbose(sprintf('adding feed stream job for %s', feedId));                            
			                                                              
			plugin.router.push('thyme/job', { _id: feedData._id, queue: process.env.SLAVE_APP_NAME,  path: 'stream/next/feed', data: { feedId: feedData._id } , label: feedData._id,  sendAt: feedData.cacheUntil.getTime() });      
			if(callback) callback();
		});
	}
	
	
	//returns feed stats
	function getFeedInfo(feedId, create, callback)
	{                         

		if(typeof feedId != 'string') throw new Error('the feed ID must be a  string');
		                        
		if(!callback)
		{
			callback = create;
			create = false;
		}       
		
		function returnFeed(err,feed)
		{
			if (feed)
			{
				var timeLeft = feed.cacheUntil.getTime() - new Date().getTime();
								
				//bug? this was added after we set the default ttl to be minimum than max IF it was set
				if(timeLeft > MAX_TTL) timeLeft = 0;

				
				if (!(feed.isOld = (timeLeft <= 0)))
				{
					// FeedTTLCacher.set (feed._id, feed.ttl, timeLeft);
				}
			}
			
			callback(err, feed);
		}            
		
		logger.verbose(sprintf("fetching %s info", feedId));

		SpiceAppFeedInfoModel.findOne({_id: feedId}, function(err, feedInfo)
		{
			if(!feedInfo && create)
			{
				var now = new Date();
				
				feedInfo = new SpiceAppFeedInfoModel({
					_id: feedId,
					ttl: 0,
					cacheUntil: now,
					// lockedUntil: new Date(now.getTime() - 1),
					watchUntil: new Date(now.getTime() + DEFAULT_TTL),
					lastUpdate: now
				});
				
				feedInfo.save();
			}	
				
			returnFeed(err, feedInfo);
		})   
	} 
	
	//used for items that may have been parsed incorrectly. In which case we don't want to ping their server
	//for any new data for quite a while...
	function setMaxTTL(feedId)
	{
		updateFeedStats(feedId, DEFAULT_TTL, undefined);
	}  
	
	//returns feed statistics based on new data, or new ttl                           
	function updateFeedStats(feedId, stackOrTTL, watchTTL, callback)
	{	                        
		//get the feed stats. Update if we have to.
		getFeedInfo(feedId, true, function(err, feedInfo)
		{   
			 var now = new Date(),
				update = {};

			if(stackOrTTL)
			{
				//get the time to live
				var ttl = Math.round(typeof stackOrTTL == 'number' ? stackOrTTL : calculateTTL(stackOrTTL, feedInfo ? feedInfo.ttl : 0));  
				
				ttl = Math.min(MAX_TTL, Math.max(ttl, MIN_TTL));

				//update the new Time To Live
				update.ttl = Number(ttl);//ttl;
				
				//last update is right now. This is used with ttl
				update.lastUpdate = now;     
				                    
				
				//we add more reduntant code so we can search against it in the database
				update.cacheUntil = new Date(now.getTime() + ttl)   
			}
			
			
			//if watch TTL is present, then we set it so the server doesn't stops after the TTL. Groups are set to a infinity and beyond
			if(watchTTL != undefined) update.watchUntil = new Date(now.getTime() + watchTTL);                    
			                           
			Structr.copy(update, feedInfo, true);
			                            

			//fucking mongoose if broken as shit.
			SpiceAppFeedInfoModel.collection.update({ _id: feedInfo.feedId }, { $set: update }, function(err)
			{
				if(callback) callback(feedInfo);
			});
		});
	}    
	       
	
	/*SpiceAppFeedInfoModel.find({ lastUpdate: { $gt: new Date(new Date().getTime() - 1000 * 3600 * 24) } }).sort('lastUpdate','descending').limit(10000).each(function (err, feedInfo)
	{
		var now = new Date().getTime();
		if(!feedInfo) return;
		
		var deleteWhen = feedInfo.cacheUntil.getTime();
		
		if(deleteWhen > now) FeedTTLCacher.set(feedInfo.feedId, deleteWhen - now, Math.min(deleteWhen - now, 3600 * 1000));
	})*/                                         
	      
	//returns useable data that's not already in the database            
	function getUseableData(feedId, newFeedItems, callback)
	{                      
		if(!newFeedItems.length) return callback(false, newFeedItems);
		
		var beginningOfTime = new Date(0);
		
		
		//let's sort the content incase it's scrambled... like eggs. We don't want eggs...
		newFeedItems.sort(function(a,b)
		{
			return (a.createdAt || beginningOfTime).getTime() - (b.createdAt || beginningOfTime).getTime();
		});
		
		var oldest = newFeedItems[0];

		
		c_feedData.find({ feedId: feedId }, { sort:[['createdAt', -1]], limit:1 }, function (err, cursor)
		{
			if(err) return callback(err)
			
			cursor.toArray(function (err, items)
			{
				if(err) console.error(err);
				if(!items) items = [];

				newestItem = items[0];

				var createdAt = newestItem ? newestItem.createdAt : false,
				useable = [];

				var one = newFeedItems[0];
				var two = newFeedItems[newFeedItems.length-1];

				for(var i = newFeedItems.length; i--;)
				{
					var item = newFeedItems[i];     

					//if the current item is OLDER than the data in the database,
					//them break the loop and return the data
					if(createdAt && item.createdAt && item.createdAt.getTime() <= createdAt.getTime()) 
					{                                  
						break;
					}


					if(!item.createdAt)
					{
						logger.warn(sprintf('item does NOT have a propper date format, skipping %s', item.link));
						continue;
					}

					item.feedId = feedId;
					useable.push(item)
				}

				if(useable.length)
				{
					logger.verbose(feedId+' has  new content.');
				}

				callback(false,useable);

			})
		})	
	}             
	
    //uses old ttl and incresed by percent if no data is found, so we ease into getting slower
	function increaseTTLOrSetDefault(oldTTL)
	{
		return oldTTL ? Math.min(oldTTL * INCREASE_PERC_TTL, MAX_TTL) : DEFAULT_TTL;
	}                                                               
	
	//calculates time to live based publish frequency
	function calculateTTL(newFeedItems, oldTTL)
	{                                   
		oldTTL = Number(oldTTL);
		                       
		//stack cannot be zero, or null, otherwise we set the OLD ttl, or the default TTL. If this continues, the OLD ttl
		//will eventually hit the max in about 6 tries. From 3600 -> 4500 -> 5625 -> 7031... 
		if(!(newFeedItems instanceof Array) || newFeedItems.length == 0) increaseTTLOrSetDefault(oldTTL);
                                                                       
		var now = new Date();
		
		var times = [];      
  
		//add the times so we can get the frequency of a new post
		for(var i = newFeedItems.length; i--;)
		{   
			var item = newFeedItems[i];  
			                         
			//skip anything without a link, and anything that doesn't have a date can go fuck themselves.
			if(!item.link || !item.createdAt) continue;        
			                                                             
			times.push(item.createdAt.getTime());
		}                           
		                                                     
		                                             
		//pop a new time on incase there's only ONE record. this won't matter if it's an outlier 
		times.push(new Date().getTime());  
		                                        
		//we want the AVERAGE TTL between posts, removing ANY outliers. No outliers because on initial load, it could
		//fudge up good times vs off times
		var averageMSNewPost = getFrequency(times);         
		

		//make sure the average time given is GREATER than the minimum time. 
		//we don't want to drown the system. Also, we really, REALLY want to get the newest feeds ASAP, otherwise OTHER feeds
		//might come in, and we'll lose shit :(. If the average time is 1 hour intervals, load in 30 minutes, and increase by 25% on each fail after. 
		var ttl = Math.max(averageMSNewPost || DEFAULT_TTL, MIN_TTL) * 0.5;
		                  
		//make sure the new ttl is LESS than the max time. we don't want the average time
		//to fool the app into loading content every 6 months     
		var newTTL = Math.min(MAX_TTL, ttl);   
		
		//FINALLY, we do NOT want the new ttl to exceed the OLD ttl. What if the post(s) were made at
		//off times such as in the middle of the night, or early morning? we don't want it messing with the original ttl, so let's 
		//ease into a longer TTL if content isn't updated in a while. This is automatically reset after a newer ttl comes up.
		if(oldTTL && newTTL > oldTTL)
		{
			return oldTTL * INCREASE_PERC_TTL;  
		}
		
		return newTTL;
	}

	dumpOld(c_feedData, 'streamed data', MAX_FEED_AGE);
}


exports.FeedDelegate = FeedDelegate;